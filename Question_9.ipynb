{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-10T01:15:43.748398Z",
     "start_time": "2026-02-10T01:15:42.810793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "WIKIPEDIA_PAGE_URL = 'https://en.wikipedia.org/wiki/Machine_learning'\n",
    "\n",
    "## For authorization\n",
    "REQUEST_HEADERS = {\n",
    "    \"User-Agent\": \"UCalgary-Student-Assignment/1.0 (educational-use)\"\n",
    "}\n",
    "\n",
    "def has_3_rows_at_least(table):\n",
    "    \"\"\"\n",
    "    Return True if the table has at least 3 data rows.\n",
    "    A \"data row\" is a <tr> that contains at least one <td>.\n",
    "    \"\"\"\n",
    "    row_count = 0\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        if row.find(\"td\"):\n",
    "            row_count += 1\n",
    "            if row_count >= 3:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean up cell text:\n",
    "    - replace non-breaking spaces\n",
    "    - collapse multiple spaces/newlines into single spaces\n",
    "    \"\"\"\n",
    "    return \" \".join(text.replace(\"\\xa0\", \" \").split())\n",
    "\n",
    "\n",
    "wikipedia_page_html = requests.get(WIKIPEDIA_PAGE_URL, headers = REQUEST_HEADERS)\n",
    "wikipedia_page_html.raise_for_status() # Check the status of the request\n",
    "\n",
    "parsed_html_document = BeautifulSoup(wikipedia_page_html.text, 'html.parser')\n",
    "\n",
    "div_content = parsed_html_document.find(\"div\", id = \"mw-content-text\")\n",
    "\n",
    "# Find the FIRST table that has at least 3 data rows\n",
    "target_table = None\n",
    "for table in div_content.find_all(\"table\"):\n",
    "    if has_3_rows_at_least(table):\n",
    "        target_table = table\n",
    "        break # stop at the first matching table\n",
    "\n",
    "if not target_table:\n",
    "    print('No matching table found')\n",
    "\n",
    "# Extract table\n",
    "tr_contents = target_table.find_all(\"tr\")\n",
    "\n",
    "data_in_trs = [tr_content for tr_content in tr_contents if tr_content.find(\"td\")]\n",
    "data_rows = []\n",
    "max_columns = 0\n",
    "\n",
    "for tr_content in data_in_trs:\n",
    "    cells = tr_content.find_all([\"th\", \"td\"])\n",
    "    row = [clean_text(cell.get_text(\" \", strip=True)) for cell in cells]\n",
    "    data_rows.append(row)\n",
    "    max_columns = max(max_columns, len(row))\n",
    "\n",
    "if max_columns == 0:\n",
    "    print(\"Found the table but couldn't extract any cells\")\n",
    "\n",
    "# Header: use a <tr> of only <th> before the first data row if it matches width\n",
    "first_data_index = tr_contents.index(data_in_trs[0])\n",
    "header = None\n",
    "\n",
    "for tr_content in tr_contents[:first_data_index]:\n",
    "    th_contents = tr_content.find_all([\"th\"])\n",
    "    td_contents = tr_content.find_all([\"td\"])\n",
    "\n",
    "    if th_contents and not td_contents and len(th_contents) == max_columns:\n",
    "        header = [clean_text(th_content.get_text(\" \", strip=True)) for th_content in th_contents]\n",
    "        break\n",
    "\n",
    "# If no header, create col1...colN\n",
    "if not header:\n",
    "    header = [f\"col{i}\" for i in range(1, max_columns + 1)]\n",
    "\n",
    "# Pad short rows with \"\"\n",
    "normalized = []\n",
    "for row in data_rows:\n",
    "    if len(row) < max_columns:\n",
    "        row = row + [\"\"] * (max_columns - len(row))\n",
    "    elif len(row) > max_columns:\n",
    "        row = row[:max_columns]\n",
    "    normalized.append(row)\n",
    "\n",
    "# Save CSV\n",
    "with open(\"wiki_table.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "    file.write(\",\".join(header) + \"\\n\")\n",
    "    for row in normalized:\n",
    "        file.write(\",\".join(row) + \"\\n\")\n",
    "\n",
    "print(\"Saved wiki_table.csv\")"
   ],
   "id": "affb2f38e30180da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved wiki_table.csv\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
